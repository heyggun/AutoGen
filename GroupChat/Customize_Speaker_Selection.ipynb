{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> **AutoGen > User Guide > GroupChat > Customize Speaker Selection**\n",
    "https://microsoft.github.io/autogen/0.2/docs/topics/groupchat/customized_speaker_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customize Speaker Selection\n",
    "\n",
    "![](https://velog.velcdn.com/images/heyggun/post/e40a770a-6225-4e6a-b020-21118d2bb9b5/image.png)\n",
    "\n",
    "GroupChat에서는 `GroupChat` 객체에 함수를 전달하여 **발화자 선택을 커스터마이징**할 수 있다. 이 함수를 사용하면 더 결정론적인 에이전트 워크플로우를 구성할 수 있으며, 이를 구현할 때는 **StateFlow 패턴**을 따르는 것을 권장한다. 자세한 내용은 StateFlow 블로그를 참고\n",
    "\n",
    "> **StateFlow**\n",
    "https://microsoft.github.io/autogen/0.2/blog/2024/02/29/StateFlow/\n",
    "**StateFlow 정리 포스팅**\n",
    "https://velog.io/@heyggun/AutoGen-StateFlow-GroupChat%EC%97%90%EC%84%9C-%EC%82%AC%EC%9A%A9%EC%9E%90-%EC%A7%80%EC%A0%95-%EB%B0%9C%ED%99%94%EC%9E%90-%EC%84%A0%ED%83%9D%EC%9D%B4-%EA%B0%80%EB%8A%A5%ED%95%9C-%EC%83%81%ED%83%9C-%EA%B8%B0%EB%B0%98-%EC%9B%8C%ED%81%AC%ED%94%8C%EB%A1%9C%EC%9A%B0-%EA%B5%AC%EC%B6%95\n",
    "\n",
    "### An example research workflow\n",
    "\n",
    "다음은 사용자 지정 발화자 선택 기능이 포함된 연구용 **StateFlow** 모델을 구축하는 간단한 예시이다. 먼저 다음과 같은 에이전트들을 정의한다.\n",
    "\n",
    "- **Initializer** (초기화자) : 작업을 전송하여 워크플로우를 시작한다.\n",
    "- **Coder** (코더) : 코드를 작성하여 인터넷에서 논문을 검색한다.\n",
    "- **Executor** (실행자) : 코드를 실행한다.\n",
    "- **Scientist** (과학자) : 논문을 읽고 요약을 작성한다.\n",
    "\n",
    "위 그림에서는 4개의 상태(`Init`, `Retrieve`, `Research`, `End`)로 구성된 간단한 연구 워크플로우를 정의한다. 각 상태에서는 다양한 에이전트들을 호출하여 작업을 수행한다.\n",
    "\n",
    "- **Init** (초기화) : *Initializer*를 사용하여 워크플로우를 시작한다.\n",
    "- **Retrieve** (검색) :  먼저 *Coder*를 호출하여 코드를 작성하고, 이후 *Executor*를 호출하여 코드를 실행한다.\n",
    "- **Research** (연구) : *Scientist*를 호출하여 논문을 읽고 요약을 작성하게 한다.\n",
    "- **End** (종료) : 워크플로우를 종료한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Create your speaker selection function\n",
    "\n",
    "다음은 `speaker selection` 함수의 뼈대이다. 이 함수에 `speaker selection` 로직을 정의하여 완성한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "from autogen import Agent, GroupChat\n",
    "from typing import Literal, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_speaker_selection_func(\n",
    "    last_speaker: Agent,\n",
    "    groupchat: GroupChat\n",
    ") -> Union[Agent, Literal[\"auto\", \"manual\", \"random\", \"round_robin\"], None]:\n",
    "    \n",
    "    \"\"\"\n",
    "    Define a customized speaker selection function.\n",
    "    A recommended way is to define a transition for each speaker in the groupchat.\n",
    "    \n",
    "    Parameters:\n",
    "        - last_speaker : Agent\n",
    "            The last speaker in the group chat.\n",
    "        - groupchat : GroupChat\n",
    "            The GroupChat object\n",
    "\n",
    "    Return:\n",
    "        Return one of the following:\n",
    "        1. an `Agent` class, it must be one of the agents in the group chat.\n",
    "        2. a string from [\"auto\", \"manual\", \"random\", \"round_roubin\"] to select a default method to use.\n",
    "        3. None, which indicates the chat should be terminated.\n",
    "    \"\"\"\n",
    "    pass\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import settings\n",
    "\n",
    "api_key = settings.openai_api_key.get_secret_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_config = {\n",
    "    \"config_list\":\n",
    "        [\n",
    "            {\n",
    "                \"model\" : \"gpt-4o-mini\",\n",
    "                \"api_key\" : api_key\n",
    "            }\n",
    "        ]\n",
    "}\n",
    "\n",
    "code_execution_config= {\n",
    "    \"last_n_messages\" : 3,\n",
    "    \"work_dir\" : \"paper\",\n",
    "    \"use_docker\" : False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import AssistantAgent, UserProxyAgent, ConversableAgent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = UserProxyAgent(\n",
    "    name=\"init\"\n",
    ")\n",
    "\n",
    "coder = autogen.AssistantAgent(\n",
    "    name=\"Retrieve_Action_1\",\n",
    "    llm_config = llm_config,\n",
    "    system_message= \"\"\"You are the Coder. Given a topic, write code to retrieve related papers form the arXiv API, print their title, authors, abstract, and link.\n",
    "You write python/shell code to solve tasks. Wrap the code in a code block that specifies the script type. The user can't modify your code. So do not suggest incomplete code which requires others to modify.\n",
    "Don't use a code block if it's not intended to be executed by the executor. \n",
    "Don't include multiple code blocks in one response. Do not ask others to copy and paste the result.\n",
    "Check the execution result return ed by the executor.\n",
    "If the result indicates there is an error, fix the error and output the code again.\n",
    "Suggest the full code instread of partial code or code changes.\n",
    "If the error can't be fixed or if the task is not solved even after the code is executed successfully, analyze the problem, revisit you assumption, collect additional info you need, and think of a different approach to try.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "executor = autogen.UserProxyAgent(\n",
    "    name=\"Retrieve_Action_2\",\n",
    "    system_message= \"Executor. Execute the code written by the Coder and report the reulst.\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    code_execution_config=code_execution_config,\n",
    ")\n",
    "\n",
    "scientist = autogen.AssistantAgent(\n",
    "    name=\"Research_Action_1\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"\"\"You are the Scientist. Please categorize papers after seeing their abstracts printed and create a markdown table with Domain, Titel, Authors, Summary and Link\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_transition(last_speaker, groupchat):\n",
    "    messages = groupchat.messages\n",
    "    \n",
    "    if last_speaker is initializer:\n",
    "        return coder\n",
    "        \n",
    "    elif last_speaker is coder:\n",
    "        return executor\n",
    "    \n",
    "    elif last_speaker is executor:\n",
    "        if messages[-1][\"content\"] == \"exitcode: 1\":\n",
    "            return coder\n",
    "        \n",
    "        else:\n",
    "            return scientist\n",
    "        \n",
    "    elif last_speaker == \"Scientist\":\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupchat = autogen.GroupChat(\n",
    "    agents=[initializer, coder, executor, scientist],\n",
    "    messages=[],\n",
    "    max_round=20,\n",
    "    speaker_selection_method=state_transition,\n",
    ")\n",
    "\n",
    "manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33minit\u001b[0m (to chat_manager):\n",
      "\n",
      "Topic : LLM applications papers from last week. Requirement : 5- 10 papers from different domains.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Retrieve_Action_1\n",
      "\u001b[0m\n",
      "\u001b[33mRetrieve_Action_1\u001b[0m (to chat_manager):\n",
      "\n",
      "```python\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "import xml.etree.ElementTree as ET\n",
      "\n",
      "def get_arxiv_papers(topic, days=7, max_results=10):\n",
      "    # Calculate the date from one week ago\n",
      "    end_date = datetime.utcnow()\n",
      "    start_date = end_date - timedelta(days=days)\n",
      "\n",
      "    # Format dates for arXiv's API query\n",
      "    start_date_str = start_date.strftime('%Y%m%d')\n",
      "    end_date_str = end_date.strftime('%Y%m%d')\n",
      "\n",
      "    # Define the arXiv API URL\n",
      "    url = f'http://export.arxiv.org/api/query?search_query=all:{topic}&start=0&max_results={max_results}&sortBy=submittedDate&sortOrder=descending'\n",
      "    \n",
      "    response = requests.get(url)\n",
      "    \n",
      "    if response.status_code == 200:\n",
      "        # Parse the XML response\n",
      "        root = ET.fromstring(response.content)\n",
      "        entries = root.findall('{http://www.w3.org/2005/Atom}entry')\n",
      "        \n",
      "        for entry in entries:\n",
      "            title = entry.find('{http://www.w3.org/2005/Atom}title').text\n",
      "            authors = [author.find('{http://www.w3.org/2005/Atom}name').text for author in entry.findall('{http://www.w3.org/2005/Atom}author')]\n",
      "            abstract = entry.find('{http://www.w3.org/2005/Atom}summary').text\n",
      "            link = entry.find('{http://www.w3.org/2005/Atom}id').text\n",
      "            \n",
      "            print(f'Title: {title}')\n",
      "            print(f'Authors: {\", \".join(authors)}')\n",
      "            print(f'Abstract: {abstract}')\n",
      "            print(f'Link: {link}\\n')\n",
      "    else:\n",
      "        print(f'Error fetching data: {response.status_code}')\n",
      "\n",
      "# Specify the topic\n",
      "get_arxiv_papers('LLM applications')\n",
      "```\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Retrieve_Action_2\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n",
      "\u001b[33mRetrieve_Action_2\u001b[0m (to chat_manager):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "Title: Learning Streaming Video Representation via Multitask Training\n",
      "Authors: Yibin Yan, Jilan Xu, Shangzhe Di, Yikun Liu, Yudi Shi, Qirui Chen, Zeqian Li, Yifei Huang, Weidi Xie\n",
      "Abstract:   Understanding continuous video streams plays a fundamental role in real-time\n",
      "applications including embodied AI and autonomous driving. Unlike offline video\n",
      "understanding, streaming video understanding requires the ability to process\n",
      "video streams frame by frame, preserve historical information, and make\n",
      "low-latency decisions.To address these challenges, our main contributions are\n",
      "three-fold. (i) We develop a novel streaming video backbone, termed as\n",
      "StreamFormer, by incorporating causal temporal attention into a pre-trained\n",
      "vision transformer. This enables efficient streaming video processing while\n",
      "maintaining image representation capability.(ii) To train StreamFormer, we\n",
      "propose to unify diverse spatial-temporal video understanding tasks within a\n",
      "multitask visual-language alignment framework. Hence, StreamFormer learns\n",
      "global semantics, temporal dynamics, and fine-grained spatial relationships\n",
      "simultaneously. (iii) We conduct extensive experiments on online action\n",
      "detection, online video instance segmentation, and video question answering.\n",
      "StreamFormer achieves competitive results while maintaining efficiency,\n",
      "demonstrating its potential for real-time applications.\n",
      "\n",
      "Link: http://arxiv.org/abs/2504.20041v1\n",
      "\n",
      "Title: AutoJudge: Judge Decoding Without Manual Annotation\n",
      "Authors: Roman Garipov, Fedor Velikonivtsev, Ruslan Svirschevski, Vage Egiazarian, Max Ryabinin\n",
      "Abstract:   We introduce AutoJudge, a framework that accelerates large language model\n",
      "(LLM) inference with task-specific lossy speculative decoding. Instead of\n",
      "matching the original model output distribution token-by-token, we identify\n",
      "which of the generated tokens affect the downstream quality of the generated\n",
      "response, relaxing the guarantee so that the \"unimportant\" tokens can be\n",
      "generated faster. Our approach relies on a semi-greedy search algorithm to test\n",
      "which of the mismatches between target and draft model should be corrected to\n",
      "preserve quality, and which ones may be skipped. We then train a lightweight\n",
      "classifier based on existing LLM embeddings to predict, at inference time,\n",
      "which mismatching tokens can be safely accepted without compromising the final\n",
      "answer quality. We test our approach with Llama 3.2 1B (draft) and Llama 3.1 8B\n",
      "(target) models on zero-shot GSM8K reasoning, where it achieves up to 1.5x more\n",
      "accepted tokens per verification cycle with under 1% degradation in answer\n",
      "accuracy compared to standard speculative decoding and over 2x with small loss\n",
      "in accuracy. When applied to the LiveCodeBench benchmark, our approach\n",
      "automatically detects other, programming-specific important tokens and shows\n",
      "similar speedups, demonstrating its ability to generalize across tasks.\n",
      "\n",
      "Link: http://arxiv.org/abs/2504.20039v1\n",
      "\n",
      "Title: Cam-2-Cam: Exploring the Design Space of Dual-Camera Interactions for\n",
      "  Smartphone-based Augmented Reality\n",
      "Authors: Brandon Woodard, Melvin He, Mose Sakashita, Jing Qian, Zainab Iftikhar, Joseph LaViola Jr\n",
      "Abstract:   Off-the-shelf smartphone-based AR systems typically use a single front-facing\n",
      "or rear-facing camera, which restricts user interactions to a narrow field of\n",
      "view and small screen size, thus reducing their practicality. We present\n",
      "\\textit{Cam-2-Cam}, an interaction concept implemented in three\n",
      "smartphone-based AR applications with interactions that span both cameras.\n",
      "Results from our qualitative analysis conducted on 30 participants presented\n",
      "two major design lessons that explore the interaction space of smartphone AR\n",
      "while maintaining critical AR interface attributes like embodiment and\n",
      "immersion: (1) \\textit{Balancing Contextual Relevance and Feedback Quality}\n",
      "serves to outline a delicate balance between implementing familiar interactions\n",
      "people do in the real world and the quality of multimodal AR responses and (2)\n",
      "\\textit{Preventing Disorientation using Simultaneous Capture and Alternating\n",
      "Cameras} which details how to prevent disorientation during AR interactions\n",
      "using the two distinct camera techniques we implemented in the paper.\n",
      "Additionally, we consider observed user assumptions or natural tendencies to\n",
      "inform future implementations of dual-camera setups for smartphone-based AR. We\n",
      "envision our design lessons as an initial pioneering step toward expanding the\n",
      "interaction space of smartphone-based AR, potentially driving broader adoption\n",
      "and overcoming limitations of single-camera AR.\n",
      "\n",
      "Link: http://arxiv.org/abs/2504.20035v1\n",
      "\n",
      "Title: More Clear, More Flexible, More Precise: A Comprehensive Oriented Object\n",
      "  Detection benchmark for UAV\n",
      "Authors: Kai Ye, Haidi Tang, Bowen Liu, Pingyang Dai, Liujuan Cao, Rongrong Ji\n",
      "Abstract:   Applications of unmanned aerial vehicle (UAV) in logistics, agricultural\n",
      "automation, urban management, and emergency response are highly dependent on\n",
      "oriented object detection (OOD) to enhance visual perception. Although existing\n",
      "datasets for OOD in UAV provide valuable resources, they are often designed for\n",
      "specific downstream tasks.Consequently, they exhibit limited generalization\n",
      "performance in real flight scenarios and fail to thoroughly demonstrate\n",
      "algorithm effectiveness in practical environments. To bridge this critical gap,\n",
      "we introduce CODrone, a comprehensive oriented object detection dataset for\n",
      "UAVs that accurately reflects real-world conditions. It also serves as a new\n",
      "benchmark designed to align with downstream task requirements, ensuring greater\n",
      "applicability and robustness in UAV-based OOD.Based on application\n",
      "requirements, we identify four key limitations in current UAV OOD datasets-low\n",
      "image resolution, limited object categories, single-view imaging, and\n",
      "restricted flight altitudes-and propose corresponding improvements to enhance\n",
      "their applicability and robustness.Furthermore, CODrone contains a broad\n",
      "spectrum of annotated images collected from multiple cities under various\n",
      "lighting conditions, enhancing the realism of the benchmark. To rigorously\n",
      "evaluate CODrone as a new benchmark and gain deeper insights into the novel\n",
      "challenges it presents, we conduct a series of experiments based on 22\n",
      "classical or SOTA methods.Our evaluation not only assesses the effectiveness of\n",
      "CODrone in real-world scenarios but also highlights key bottlenecks and\n",
      "opportunities to advance OOD in UAV applications.Overall, CODrone fills the\n",
      "data gap in OOD from UAV perspective and provides a benchmark with enhanced\n",
      "generalization capability, better aligning with practical applications and\n",
      "future algorithm development.\n",
      "\n",
      "Link: http://arxiv.org/abs/2504.20032v1\n",
      "\n",
      "Title: All-Subsets Important Separators with Applications to Sample Sets,\n",
      "  Balanced Separators and Vertex Sparsifiers in Directed Graphs\n",
      "Authors: Aditya Anand, Euiwoong Lee, Jason Li, Thatchaphol Saranurak\n",
      "Abstract:   Given a directed graph $G$ with $n$ vertices and $m$ edges, a parameter $k$\n",
      "and two disjoint subsets $S,T \\subseteq V(G)$, we show that the number of\n",
      "all-subsets important separators, which is the number of $A$-$B$ important\n",
      "vertex separators of size at most $k$ over all $A \\subseteq S$ and $B \\subseteq\n",
      "T$, is at most $\\beta(|S|, |T|, k) = 4^k {|S| \\choose \\leq k} {|T| \\choose \\leq\n",
      "2k}$, where ${x \\choose \\leq c} = \\sum_{i = 1}^c {x \\choose i}$, and that they\n",
      "can be enumerated in time $O(\\beta(|S|,|T|,k)k^2(m+n))$. This is a\n",
      "generalization of the folklore result stating that the number of $A$-$B$\n",
      "important separators for two fixed sets $A$ and $B$ is at most $4^k$ (first\n",
      "implicitly shown by Chen, Liu and Lu Algorithmica '09). From this result, we\n",
      "obtain the following applications: We give a construction for detection sets\n",
      "and sample sets in directed graphs, generalizing the results of Kleinberg\n",
      "(Internet Mathematics' 03) and Feige and Mahdian (STOC' 06) to directed graphs.\n",
      "Via our new sample sets, we give the first FPT algorithm for finding balanced\n",
      "separators in directed graphs parameterized by $k$, the size of the separator.\n",
      "Our algorithm runs in time $2^{O(k)} (m + n)$. We also give a $O({\\sqrt{\\log\n",
      "k}})$ approximation algorithm for the same problem. Finally, we present new\n",
      "results on vertex sparsifiers for preserving small cuts.\n",
      "\n",
      "Link: http://arxiv.org/abs/2504.20027v1\n",
      "\n",
      "Title: Better To Ask in English? Evaluating Factual Accuracy of Multilingual\n",
      "  LLMs in English and Low-Resource Languages\n",
      "Authors: Pritika Rohera, Chaitrali Ginimav, Gayatri Sawant, Raviraj Joshi\n",
      "Abstract:   Multilingual Large Language Models (LLMs) have demonstrated significant\n",
      "effectiveness across various languages, particularly in high-resource languages\n",
      "such as English. However, their performance in terms of factual accuracy across\n",
      "other low-resource languages, especially Indic languages, remains an area of\n",
      "investigation. In this study, we assess the factual accuracy of LLMs - GPT-4o,\n",
      "Gemma-2-9B, Gemma-2-2B, and Llama-3.1-8B - by comparing their performance in\n",
      "English and Indic languages using the IndicQuest dataset, which contains\n",
      "question-answer pairs in English and 19 Indic languages. By asking the same\n",
      "questions in English and their respective Indic translations, we analyze\n",
      "whether the models are more reliable for regional context questions in Indic\n",
      "languages or when operating in English. Our findings reveal that LLMs often\n",
      "perform better in English, even for questions rooted in Indic contexts.\n",
      "Notably, we observe a higher tendency for hallucination in responses generated\n",
      "in low-resource Indic languages, highlighting challenges in the multilingual\n",
      "understanding capabilities of current LLMs.\n",
      "\n",
      "Link: http://arxiv.org/abs/2504.20022v1\n",
      "\n",
      "Title: Modular Machine Learning: An Indispensable Path towards New-Generation\n",
      "  Large Language Models\n",
      "Authors: Xin Wang, Haoyang Li, Zeyang Zhang, Haibo Chen, Wenwu Zhu\n",
      "Abstract:   Large language models (LLMs) have dramatically advanced machine learning\n",
      "research including natural language processing, computer vision, data mining,\n",
      "etc., yet they still exhibit critical limitations in reasoning, factual\n",
      "consistency, and interpretability. In this paper, we introduce a novel learning\n",
      "paradigm -- Modular Machine Learning (MML) -- as an essential approach toward\n",
      "new-generation LLMs. MML decomposes the complex structure of LLMs into three\n",
      "interdependent components: modular representation, modular model, and modular\n",
      "reasoning, aiming to enhance LLMs' capability of counterfactual reasoning,\n",
      "mitigating hallucinations, as well as promoting fairness, safety, and\n",
      "transparency. Specifically, the proposed MML paradigm can: i) clarify the\n",
      "internal working mechanism of LLMs through the disentanglement of semantic\n",
      "components; ii) allow for flexible and task-adaptive model design; iii) enable\n",
      "interpretable and logic-driven decision-making process. We present a feasible\n",
      "implementation of MML-based LLMs via leveraging advanced techniques such as\n",
      "disentangled representation learning, neural architecture search and\n",
      "neuro-symbolic learning. We critically identify key challenges, such as the\n",
      "integration of continuous neural and discrete symbolic processes, joint\n",
      "optimization, and computational scalability, present promising future research\n",
      "directions that deserve further exploration. Ultimately, the integration of the\n",
      "MML paradigm with LLMs has the potential to bridge the gap between statistical\n",
      "(deep) learning and formal (logical) reasoning, thereby paving the way for\n",
      "robust, adaptable, and trustworthy AI systems across a wide range of real-world\n",
      "applications.\n",
      "\n",
      "Link: http://arxiv.org/abs/2504.20020v1\n",
      "\n",
      "Title: MINT: Multi-Vector Search Index Tuning\n",
      "Authors: Jiongli Zhu, Yue Wang, Bailu Ding, Philip A. Bernstein, Vivek Narasayya, Surajit Chaudhuri\n",
      "Abstract:   Vector search plays a crucial role in many real-world applications. In\n",
      "addition to single-vector search, multi-vector search becomes important for\n",
      "multi-modal and multi-feature scenarios today. In a multi-vector database, each\n",
      "row is an item, each column represents a feature of items, and each cell is a\n",
      "high-dimensional vector. In multi-vector databases, the choice of indexes can\n",
      "have a significant impact on performance. Although index tuning for relational\n",
      "databases has been extensively studied, index tuning for multi-vector search\n",
      "remains unclear and challenging. In this paper, we define multi-vector search\n",
      "index tuning and propose a framework to solve it. Specifically, given a\n",
      "multi-vector search workload, we develop algorithms to find indexes that\n",
      "minimize latency and meet storage and recall constraints. Compared to the\n",
      "baseline, our latency achieves 2.1X to 8.3X speedup.\n",
      "\n",
      "Link: http://arxiv.org/abs/2504.20018v1\n",
      "\n",
      "Title: Applying LLM-Powered Virtual Humans to Child Interviews in\n",
      "  Child-Centered Design\n",
      "Authors: Linshi Li, Hanlin Cai\n",
      "Abstract:   In child-centered design, directly engaging children is crucial for deeply\n",
      "understanding their experiences. However, current research often prioritizes\n",
      "adult perspectives, as interviewing children involves unique challenges such as\n",
      "environmental sensitivities and the need for trust-building. AI-powered virtual\n",
      "humans (VHs) offer a promising approach to facilitate engaging and multimodal\n",
      "interactions with children. This study establishes key design guidelines for\n",
      "LLM-powered virtual humans tailored to child interviews, standardizing\n",
      "multimodal elements including color schemes, voice characteristics, facial\n",
      "features, expressions, head movements, and gestures. Using ChatGPT-based prompt\n",
      "engineering, we developed three distinct Human-AI workflows (LLM-Auto,\n",
      "LLM-Interview, and LLM-Analyze) and conducted a user study involving 15\n",
      "children aged 6 to 12. The results indicated that the LLM-Analyze workflow\n",
      "outperformed the others by eliciting longer responses, achieving higher user\n",
      "experience ratings, and promoting more effective child engagement.\n",
      "\n",
      "Link: http://arxiv.org/abs/2504.20016v1\n",
      "\n",
      "Title: LLM-Generated Fake News Induces Truth Decay in News Ecosystem: A Case\n",
      "  Study on Neural News Recommendation\n",
      "Authors: Beizhe Hu, Qiang Sheng, Juan Cao, Yang Li, Danding Wang\n",
      "Abstract:   Online fake news moderation now faces a new challenge brought by the\n",
      "malicious use of large language models (LLMs) in fake news production. Though\n",
      "existing works have shown LLM-generated fake news is hard to detect from an\n",
      "individual aspect, it remains underexplored how its large-scale release will\n",
      "impact the news ecosystem. In this study, we develop a simulation pipeline and\n",
      "a dataset with ~56k generated news of diverse types to investigate the effects\n",
      "of LLM-generated fake news within neural news recommendation systems. Our\n",
      "findings expose a truth decay phenomenon, where real news is gradually losing\n",
      "its advantageous position in news ranking against fake news as LLM-generated\n",
      "news is involved in news recommendation. We further provide an explanation\n",
      "about why truth decay occurs from a familiarity perspective and show the\n",
      "positive correlation between perplexity and news ranking. Finally, we discuss\n",
      "the threats of LLM-generated fake news and provide possible countermeasures. We\n",
      "urge stakeholders to address this emerging challenge to preserve the integrity\n",
      "of news ecosystems.\n",
      "\n",
      "Link: http://arxiv.org/abs/2504.20013v1\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Research_Action_1\n",
      "\u001b[0m\n",
      "\u001b[33mResearch_Action_1\u001b[0m (to chat_manager):\n",
      "\n",
      "Here is the markdown table summarizing the LLM applications papers retrieved from the last week:\n",
      "\n",
      "| Domain               | Title                                                                                                                   | Authors                                                                                                                   | Summary                                                                                                                                                                                                                                                                                                                                                                                                                                         | Link                                       |\n",
      "|----------------------|-------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------|\n",
      "| Multimedia           | Learning Streaming Video Representation via Multitask Training                                                           | Yibin Yan, Jilan Xu, Shangzhe Di, Yikun Liu, Yudi Shi, Qirui Chen, Zeqian Li, Yifei Huang, Weidi Xie                  | This paper proposes a streaming video backbone named StreamFormer that integrates causal temporal attention for efficient real-time video understanding, maintaining historical context and low-latency decisions. It also unifies various spatial-temporal tasks within a multitask framework, demonstrating efficient performance across online video tasks.                                                                           | [Link](http://arxiv.org/abs/2504.20041v1) |\n",
      "| Natural Language     | AutoJudge: Judge Decoding Without Manual Annotation                                                                     | Roman Garipov, Fedor Velikonivtsev, Ruslan Svirschevski, Vage Egiazarian, Max Ryabinin                                  | AutoJudge accelerates LLM inference with lossy speculative decoding by identifying and skipping \"unimportant\" tokens, achieving significant speedups while maintaining answer quality. The framework demonstrates effectiveness across coding tasks, suggesting it offers a promising generalization across various tasks.                                                                                     | [Link](http://arxiv.org/abs/2504.20039v1) |\n",
      "| Augmented Reality     | Cam-2-Cam: Exploring the Design Space of Dual-Camera Interactions for Smartphone-based Augmented Reality                | Brandon Woodard, Melvin He, Mose Sakashita, Jing Qian, Zainab Iftikhar, Joseph LaViola Jr                             | This work introduces Cam-2-Cam dual-camera interactions for smartphone AR, highlighting design lessons that enhance user experience by balancing feedback quality and preventing disorientation. It aims to broaden the interaction possibilities of smartphone AR systems.                                                                                                                                                    | [Link](http://arxiv.org/abs/2504.20035v1) |\n",
      "| UAV Detection        | More Clear, More Flexible, More Precise: A Comprehensive Oriented Object Detection benchmark for UAV                   | Kai Ye, Haidi Tang, Bowen Liu, Pingyang Dai, Liujuan Cao, Rongrong Ji                                                  | The paper presents CODrone, an oriented object detection dataset designed for UAVs that addresses limitations of current datasets while providing a new benchmark for enhanced realism and applicability. It serves to improve algorithm performance in real-world UAV applications, discussing challenges and future opportunities in oriented object detection.                                                               | [Link](http://arxiv.org/abs/2504.20032v1) |\n",
      "| Graph Theory         | All-Subsets Important Separators with Applications to Sample Sets, Balanced Separators and Vertex Sparsifiers in Directed Graphs | Aditya Anand, Euiwoong Lee, Jason Li, Thatchaphol Saranurak                                                                | This study generalizes important node separator findings in directed graphs, elaborating new constructions for detection and sample sets. It shares implications for finding balanced separators and presents algorithms impacting vertex sparsifiers while enhancing understanding of vertex relationships in directed graph structures.                                                                                  | [Link](http://arxiv.org/abs/2504.20027v1) |\n",
      "| Multilingual Models  | Better To Ask in English? Evaluating Factual Accuracy of Multilingual LLMs in English and Low-Resource Languages        | Pritika Rohera, Chaitrali Ginimav, Gayatri Sawant, Raviraj Joshi                                                        | The study investigates the factual accuracy of multilingual LLMs by comparing their performance in English versus low-resource languages. The findings indicate that models often perform better in English, revealing challenges in low-resource languages, including a higher hallucination rate in responses.                                                                                                  | [Link](http://arxiv.org/abs/2504.20022v1) |\n",
      "| Modular ML           | Modular Machine Learning: An Indispensable Path towards New-Generation Large Language Models                          | Xin Wang, Haoyang Li, Zeyang Zhang, Haibo Chen, Wenwu Zhu                                                               | This paper presents a Modular Machine Learning paradigm, dissecting LLMs into components to enhance reasoning, transparency, and flexibility. It suggests deep integrations of neural and symbolic processes, urging future research on overcoming challenges in model design and interpretability.                                                                                                                                     | [Link](http://arxiv.org/abs/2504.20020v1) |\n",
      "| Multi-Vector Search  | MINT: Multi-Vector Search Index Tuning                                                                                    | Jiongli Zhu, Yue Wang, Bailu Ding, Philip A. Bernstein, Vivek Narasayya, Surajit Chaudhuri                             | The authors define multi-vector search index tuning for improving search performance in multi-vector databases. The proposed framework aims to optimize indexes that significantly reduce latency compared to baseline methods, paving the way for better performance in multi-modal applications.                                                                                                       | [Link](http://arxiv.org/abs/2504.20018v1) |\n",
      "| Child Psychology     | Applying LLM-Powered Virtual Humans to Child Interviews in Child-Centered Design                                         | Linshi Li, Hanlin Cai                                                                                                   | This study demonstrates the use of LLM-powered virtual humans tailored for child-centered design interviews. It sets design guidelines and shows how LLM workflows can enhance child engagement during interviews, outperforming traditional methods in terms of eliciting responses and user experiences.                                                                                                         | [Link](http://arxiv.org/abs/2504.20016v1) |\n",
      "| News Ecosystem       | LLM-Generated Fake News Induces Truth Decay in News Ecosystem: A Case Study on Neural News Recommendation              | Beizhe Hu, Qiang Sheng, Juan Cao, Yang Li, Danding Wang                                                                   | This research explores the impact of LLM-generated fake news on the news recommendation ecosystem, revealing a \"truth decay\" effect. It highlights the challenges of moderating fake news and suggests countermeasures to preserve news integrity, showing how generated content may disrupt real news ranking within recommendation systems.                                                                 | [Link](http://arxiv.org/abs/2504.20013v1) | \n",
      "\n",
      "Feel free to use or modify this table for your needs!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> TERMINATING RUN (c8b7aa31-32e1-47e6-909c-bef2495440b9): No next speaker selected\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "result = initializer.initiate_chat(\n",
    "    manager,\n",
    "    message=\"Topic : LLM applications papers from last week. Requirement : 5- 10 papers from different domains.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Here is the markdown table summarizing the LLM applications papers retrieved '\n",
      " 'from the last week:\\n'\n",
      " '\\n'\n",
      " '| Domain               | '\n",
      " 'Title                                                                                                                   '\n",
      " '| '\n",
      " 'Authors                                                                                                                   '\n",
      " '| '\n",
      " 'Summary                                                                                                                                                                                                                                                                                                                                                                                                                                         '\n",
      " '| Link                                       |\\n'\n",
      " '|----------------------|-------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------|\\n'\n",
      " '| Multimedia           | Learning Streaming Video Representation via '\n",
      " 'Multitask Training                                                           '\n",
      " '| Yibin Yan, Jilan Xu, Shangzhe Di, Yikun Liu, Yudi Shi, Qirui Chen, Zeqian '\n",
      " 'Li, Yifei Huang, Weidi Xie                  | This paper proposes a '\n",
      " 'streaming video backbone named StreamFormer that integrates causal temporal '\n",
      " 'attention for efficient real-time video understanding, maintaining '\n",
      " 'historical context and low-latency decisions. It also unifies various '\n",
      " 'spatial-temporal tasks within a multitask framework, demonstrating efficient '\n",
      " 'performance across online video '\n",
      " 'tasks.                                                                           '\n",
      " '| [Link](http://arxiv.org/abs/2504.20041v1) |\\n'\n",
      " '| Natural Language     | AutoJudge: Judge Decoding Without Manual '\n",
      " 'Annotation                                                                     '\n",
      " '| Roman Garipov, Fedor Velikonivtsev, Ruslan Svirschevski, Vage Egiazarian, '\n",
      " 'Max Ryabinin                                  | AutoJudge accelerates LLM '\n",
      " 'inference with lossy speculative decoding by identifying and skipping '\n",
      " '\"unimportant\" tokens, achieving significant speedups while maintaining '\n",
      " 'answer quality. The framework demonstrates effectiveness across coding '\n",
      " 'tasks, suggesting it offers a promising generalization across various '\n",
      " 'tasks.                                                                                     '\n",
      " '| [Link](http://arxiv.org/abs/2504.20039v1) |\\n'\n",
      " '| Augmented Reality     | Cam-2-Cam: Exploring the Design Space of '\n",
      " 'Dual-Camera Interactions for Smartphone-based Augmented '\n",
      " 'Reality                | Brandon Woodard, Melvin He, Mose Sakashita, Jing '\n",
      " 'Qian, Zainab Iftikhar, Joseph LaViola Jr                             | This '\n",
      " 'work introduces Cam-2-Cam dual-camera interactions for smartphone AR, '\n",
      " 'highlighting design lessons that enhance user experience by balancing '\n",
      " 'feedback quality and preventing disorientation. It aims to broaden the '\n",
      " 'interaction possibilities of smartphone AR '\n",
      " 'systems.                                                                                                                                                    '\n",
      " '| [Link](http://arxiv.org/abs/2504.20035v1) |\\n'\n",
      " '| UAV Detection        | More Clear, More Flexible, More Precise: A '\n",
      " 'Comprehensive Oriented Object Detection benchmark for UAV                   '\n",
      " '| Kai Ye, Haidi Tang, Bowen Liu, Pingyang Dai, Liujuan Cao, Rongrong '\n",
      " 'Ji                                                  | The paper presents '\n",
      " 'CODrone, an oriented object detection dataset designed for UAVs that '\n",
      " 'addresses limitations of current datasets while providing a new benchmark '\n",
      " 'for enhanced realism and applicability. It serves to improve algorithm '\n",
      " 'performance in real-world UAV applications, discussing challenges and future '\n",
      " 'opportunities in oriented object '\n",
      " 'detection.                                                               | '\n",
      " '[Link](http://arxiv.org/abs/2504.20032v1) |\\n'\n",
      " '| Graph Theory         | All-Subsets Important Separators with Applications '\n",
      " 'to Sample Sets, Balanced Separators and Vertex Sparsifiers in Directed '\n",
      " 'Graphs | Aditya Anand, Euiwoong Lee, Jason Li, Thatchaphol '\n",
      " 'Saranurak                                                                | '\n",
      " 'This study generalizes important node separator findings in directed graphs, '\n",
      " 'elaborating new constructions for detection and sample sets. It shares '\n",
      " 'implications for finding balanced separators and presents algorithms '\n",
      " 'impacting vertex sparsifiers while enhancing understanding of vertex '\n",
      " 'relationships in directed graph '\n",
      " 'structures.                                                                                  '\n",
      " '| [Link](http://arxiv.org/abs/2504.20027v1) |\\n'\n",
      " '| Multilingual Models  | Better To Ask in English? Evaluating Factual '\n",
      " 'Accuracy of Multilingual LLMs in English and Low-Resource Languages        | '\n",
      " 'Pritika Rohera, Chaitrali Ginimav, Gayatri Sawant, Raviraj '\n",
      " 'Joshi                                                        | The study '\n",
      " 'investigates the factual accuracy of multilingual LLMs by comparing their '\n",
      " 'performance in English versus low-resource languages. The findings indicate '\n",
      " 'that models often perform better in English, revealing challenges in '\n",
      " 'low-resource languages, including a higher hallucination rate in '\n",
      " 'responses.                                                                                                  '\n",
      " '| [Link](http://arxiv.org/abs/2504.20022v1) |\\n'\n",
      " '| Modular ML           | Modular Machine Learning: An Indispensable Path '\n",
      " 'towards New-Generation Large Language Models                          | Xin '\n",
      " 'Wang, Haoyang Li, Zeyang Zhang, Haibo Chen, Wenwu '\n",
      " 'Zhu                                                               | This '\n",
      " 'paper presents a Modular Machine Learning paradigm, dissecting LLMs into '\n",
      " 'components to enhance reasoning, transparency, and flexibility. It suggests '\n",
      " 'deep integrations of neural and symbolic processes, urging future research '\n",
      " 'on overcoming challenges in model design and '\n",
      " 'interpretability.                                                                                                                                     '\n",
      " '| [Link](http://arxiv.org/abs/2504.20020v1) |\\n'\n",
      " '| Multi-Vector Search  | MINT: Multi-Vector Search Index '\n",
      " 'Tuning                                                                                    '\n",
      " '| Jiongli Zhu, Yue Wang, Bailu Ding, Philip A. Bernstein, Vivek Narasayya, '\n",
      " 'Surajit Chaudhuri                             | The authors define '\n",
      " 'multi-vector search index tuning for improving search performance in '\n",
      " 'multi-vector databases. The proposed framework aims to optimize indexes that '\n",
      " 'significantly reduce latency compared to baseline methods, paving the way '\n",
      " 'for better performance in multi-modal '\n",
      " 'applications.                                                                                                       '\n",
      " '| [Link](http://arxiv.org/abs/2504.20018v1) |\\n'\n",
      " '| Child Psychology     | Applying LLM-Powered Virtual Humans to Child '\n",
      " 'Interviews in Child-Centered Design                                         '\n",
      " '| Linshi Li, Hanlin '\n",
      " 'Cai                                                                                                   '\n",
      " '| This study demonstrates the use of LLM-powered virtual humans tailored for '\n",
      " 'child-centered design interviews. It sets design guidelines and shows how '\n",
      " 'LLM workflows can enhance child engagement during interviews, outperforming '\n",
      " 'traditional methods in terms of eliciting responses and user '\n",
      " 'experiences.                                                                                                         '\n",
      " '| [Link](http://arxiv.org/abs/2504.20016v1) |\\n'\n",
      " '| News Ecosystem       | LLM-Generated Fake News Induces Truth Decay in News '\n",
      " 'Ecosystem: A Case Study on Neural News Recommendation              | Beizhe '\n",
      " 'Hu, Qiang Sheng, Juan Cao, Yang Li, Danding '\n",
      " 'Wang                                                                   | '\n",
      " 'This research explores the impact of LLM-generated fake news on the news '\n",
      " 'recommendation ecosystem, revealing a \"truth decay\" effect. It highlights '\n",
      " 'the challenges of moderating fake news and suggests countermeasures to '\n",
      " 'preserve news integrity, showing how generated content may disrupt real news '\n",
      " 'ranking within recommendation '\n",
      " 'systems.                                                                 | '\n",
      " '[Link](http://arxiv.org/abs/2504.20013v1) | \\n'\n",
      " '\\n'\n",
      " 'Feel free to use or modify this table for your needs!')\n"
     ]
    }
   ],
   "source": [
    "pprint(result.chat_history[-1]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "user-prediction-jY4gRI11-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
