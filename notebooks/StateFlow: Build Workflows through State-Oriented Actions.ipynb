{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> **AutoGen > Notebooks > StateFlow**\n",
    "https://microsoft.github.io/autogen/0.2/docs/notebooks/agentchat_groupchat_stateflow/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StateFlow : Build Workflows through State-Oriented Actions\n",
    "\n",
    "**`AutoGen`**은 LLM(대형 언어 모델), 도구 또는 사람에 의해 구동되는 대화형 에이전트를 제공하며, 이들은 자동화된 채팅을 통해 함께 작업을 수행할 수 있도록 구성할 수 있다. 이 notebook에서는 AutoGen 에이전트를 사용해 **state**, 즉 상태 중심의 관점에서 *Groupchat*으로 워크플로우를 구축하는 방법을 소개 한다.\n",
    "\n",
    "! `autogen-agentchat` 이 설치 되어 있어야 한다.\n",
    "```python\n",
    "pip install autogen-agentchat~=0.2\n",
    "```\n",
    "\n",
    "설치에 대한 더 자세한 정보는 아래의 **Installation guide**를 참고\n",
    "\n",
    "> **Installation guide**\n",
    "https://microsoft.github.io/autogen/0.2/docs/installation/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See your API Endpoint\n",
    "`config_list_from_json` 함수는 환경 변수 또는 JSON 파일에서 구성 목록을 로드한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import settings\n",
    "\n",
    "api_key = settings.openai_api_key.get_secret_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "\n",
    "config_list = autogen.config_list_from_json(\n",
    "    \"/Users/geonheekim/Desktop/python_interview/autoGen/notebooks/ config_list.json\",\n",
    "    filter_dict={\n",
    "        \"tags\" : [\"gpt-4o-mini\"],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'model': 'gpt-4o-mini', 'tags': ['gpt-4o-mini']}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A workflow for research\n",
    "\n",
    "연구를 위한 워크플로우를 위하여 다음과 같은 에이전트를 정의한다. 작업을 전달하여 워크플로우를 시작하는 **Initalizer(초기화자)**, 인터넷에서 논문을 검색할 수 있는 코드를 작성하는 **Coder(코더)**, 코드를 실행하는 **Executor(실행자)**,논문을 읽고 요약을 작성하는 **Scientist(과학자)**\n",
    "\n",
    "4개의 상태(`Init`, `Retrieve`, `Research`, `End`)를 갖는 간단한 연구 플로우를 정의하고, 각 상태에서는 서로 다른 에이전트를 호출하여 작업을 수행한다.\n",
    "\n",
    "- **Init** : *Initializer*를 사용해 워크플로우를 시작\n",
    "- **Retrieve** : 먼저 Coder를 호출해 코드를 작성하고, 그 다음 *Executor*를 호출하여 코드를 실행한다.\n",
    "- **Research** : *Scientist*를 호출하여 논문을 읽고 요약을 작성한다.\n",
    "- **End** : 워크플로우를 종료한다.\n",
    "\n",
    "에이전트 간의 전환을 정의함으로써 speaker selection method(화자 선택 방법)을 사용자 정의하여 상태 중심 워크플로우를 손쉽게 구현할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from autogen.coding import LocalCommandLineCodeExecutor\n",
    "from config import settings\n",
    "\n",
    "api_key = settings.openai_api_key.get_secret_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dir = tempfile.TemporaryDirectory()\n",
    "executor = LocalCommandLineCodeExecutor(\n",
    "    timeout=10,\n",
    "    work_dir=temp_dir.name)\n",
    "\n",
    "config_list = {\n",
    "    \"config_list\":\n",
    "        [\n",
    "            {\n",
    "                \"model\" : \"gpt-4o-mini\",\n",
    "                \"api_key\" : api_key\n",
    "            }\n",
    "        ]\n",
    "}\n",
    "\n",
    "gpt4o_mini_config = {\n",
    "    \"cache_seed\" : False,\n",
    "    \"temperature\" : 0,\n",
    "   \"config_list\" : config_list[\"config_list\"],\n",
    "   \"timeout\" : 120,\n",
    "   }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = autogen.AssistantAgent(\n",
    "    name=\"Init\",\n",
    "    code_execution_config=False,\n",
    ")\n",
    "\n",
    "coder = autogen.AssistantAgent(\n",
    "    name=\"Retrieve_Action_1\",\n",
    "    llm_config=gpt4o_mini_config,\n",
    "    system_message = \"\"\"You are the Coder. Given a topic, write code to retrieve related papers from the arXiv API, print their title, authors, abstract, and link.\n",
    "You write python/shell code to solve tasks. Wrap the code in a code block that specifies the script type.\n",
    "The user can't modify your code. So do not suggest incomplete code which requires others to modify.\n",
    "Don't use a code block if it's not intended to be executed by the executor.\n",
    "Don't include multiple code blocks in one response. Do not ask others to copy and paste the result.\n",
    "Check the execution result returned by the executor.\n",
    "If the result indicates there is an error, fix the error and output the code again.\n",
    "Suggest the full code instead of partial code or code changes.\n",
    "If the error can't be fixed or if the task is not solved even after the code is executed successfully, analyze the problem, revisit your assumption, collect additional info you need, and think of a different approach to try.\n",
    "\"\"\" \n",
    ")\n",
    "\n",
    "executor = autogen.AssistantAgent(\n",
    "    name=\"Retrieve_Action_2\",\n",
    "    system_message=\"Executor. Execute the code written by the Coder and report the result.\",\n",
    "    human_input_mode= \"NEVER\",\n",
    "    code_execution_config= {\"executor\" : executor},\n",
    ")\n",
    "\n",
    "scientist = autogen.AssistantAgent(\n",
    "    name=\"Research_Action_1\",\n",
    "    llm_config=gpt4o_mini_config,\n",
    "    system_message=\"\"\"You are the Scientist. \n",
    "Please categorize papers after seeing their abstracts printed and create a markdown table with Domain, Title, Authors, Summary and Link\"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_transition(last_speaker, groupchat):\n",
    "    messages = groupchat.messages\n",
    "    \n",
    "    if last_speaker == initializer:\n",
    "        return coder\n",
    "    elif last_speaker is coder:\n",
    "        return executor\n",
    "    elif last_speaker is executor:\n",
    "        if messages[-1][\"content\"] == \"exitcode: 1\":\n",
    "            return coder\n",
    "        else:\n",
    "            return scientist\n",
    "    elif last_speaker == \"Scientist\":\n",
    "        return None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupchat = autogen.GroupChat(\n",
    "    agents=[initializer, coder, executor, scientist],\n",
    "    messages=[],\n",
    "    max_round=20,\n",
    "    speaker_selection_method = state_transition,\n",
    ")\n",
    "\n",
    "manager = autogen.GroupChatManager(\n",
    "    groupchat= groupchat,\n",
    "    llm_config = gpt4o_mini_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mInit\u001b[0m (to chat_manager):\n",
      "\n",
      "Topic: LLM applications papers from last week. Requirement: 5 -10 papers from different domains.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Retrieve_Action_1\n",
      "\u001b[0m\n",
      "\u001b[33mRetrieve_Action_1\u001b[0m (to chat_manager):\n",
      "\n",
      "```python\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "import xml.etree.ElementTree as ET\n",
      "\n",
      "def fetch_arxiv_papers(topic, days=7, max_results=10):\n",
      "    # Calculate the date range for the last week\n",
      "    end_date = datetime.now()\n",
      "    start_date = end_date - timedelta(days=days)\n",
      "    \n",
      "    # Format dates for arXiv query\n",
      "    start_date_str = start_date.strftime('%Y%m%d')\n",
      "    end_date_str = end_date.strftime('%Y%m%d')\n",
      "    \n",
      "    # Construct the arXiv API query\n",
      "    query = f\"search_query=all:{topic}&start=0&max_results={max_results}&sortBy=submittedDate&sortOrder=descending&dateRange={start_date_str}-{end_date_str}\"\n",
      "    url = f\"http://export.arxiv.org/api/query?{query}\"\n",
      "    \n",
      "    # Make the request to the arXiv API\n",
      "    response = requests.get(url)\n",
      "    \n",
      "    # Parse the XML response\n",
      "    root = ET.fromstring(response.content)\n",
      "    \n",
      "    # Extract and print paper details\n",
      "    for entry in root.findall('{http://www.w3.org/2005/Atom}entry'):\n",
      "        title = entry.find('{http://www.w3.org/2005/Atom}title').text\n",
      "        authors = ', '.join(author.find('{http://www.w3.org/2005/Atom}name').text for author in entry.findall('{http://www.w3.org/2005/Atom}author'))\n",
      "        abstract = entry.find('{http://www.w3.org/2005/Atom}summary').text\n",
      "        link = entry.find('{http://www.w3.org/2005/Atom}id').text\n",
      "        \n",
      "        print(f\"Title: {title}\")\n",
      "        print(f\"Authors: {authors}\")\n",
      "        print(f\"Abstract: {abstract}\")\n",
      "        print(f\"Link: {link}\\n\")\n",
      "\n",
      "# Fetch LLM applications papers from the last week\n",
      "fetch_arxiv_papers(\"LLM applications\")\n",
      "```\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Retrieve_Action_2\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK (inferred language is python)...\u001b[0m\n",
      "\u001b[33mRetrieve_Action_2\u001b[0m (to chat_manager):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: Title: Learning Streaming Video Representation via Multitask Training\n",
      "Authors: Yibin Yan, Jilan Xu, Shangzhe Di, Yikun Liu, Yudi Shi, Qirui Chen, Zeqian Li, Yifei Huang, Weidi Xie\n",
      "Abstract:   Understanding continuous video streams plays a fundamental role in real-time\n",
      "applications including embodied AI and autonomous driving. Unlike offline video\n",
      "understanding, streaming video understanding requires the ability to process\n",
      "video streams frame by frame, preserve historical information, and make\n",
      "low-latency decisions.To address these challenges, our main contributions are\n",
      "three-fold. (i) We develop a novel streaming video backbone, termed as\n",
      "StreamFormer, by incorporating causal temporal attention into a pre-trained\n",
      "vision transformer. This enables efficient streaming video processing while\n",
      "maintaining image representation capability.(ii) To train StreamFormer, we\n",
      "propose to unify diverse spatial-temporal video understanding tasks within a\n",
      "multitask visual-language alignment framework. Hence, StreamFormer learns\n",
      "global semantics, temporal dynamics, and fine-grained spatial relationships\n",
      "simultaneously. (iii) We conduct extensive experiments on online action\n",
      "detection, online video instance segmentation, and video question answering.\n",
      "StreamFormer achieves competitive results while maintaining efficiency,\n",
      "demonstrating its potential for real-time applications.\n",
      "\n",
      "Link: http://arxiv.org/abs/2504.20041v1\n",
      "\n",
      "Title: AutoJudge: Judge Decoding Without Manual Annotation\n",
      "Authors: Roman Garipov, Fedor Velikonivtsev, Ruslan Svirschevski, Vage Egiazarian, Max Ryabinin\n",
      "Abstract:   We introduce AutoJudge, a framework that accelerates large language model\n",
      "(LLM) inference with task-specific lossy speculative decoding. Instead of\n",
      "matching the original model output distribution token-by-token, we identify\n",
      "which of the generated tokens affect the downstream quality of the generated\n",
      "response, relaxing the guarantee so that the \"unimportant\" tokens can be\n",
      "generated faster. Our approach relies on a semi-greedy search algorithm to test\n",
      "which of the mismatches between target and draft model should be corrected to\n",
      "preserve quality, and which ones may be skipped. We then train a lightweight\n",
      "classifier based on existing LLM embeddings to predict, at inference time,\n",
      "which mismatching tokens can be safely accepted without compromising the final\n",
      "answer quality. We test our approach with Llama 3.2 1B (draft) and Llama 3.1 8B\n",
      "(target) models on zero-shot GSM8K reasoning, where it achieves up to 1.5x more\n",
      "accepted tokens per verification cycle with under 1% degradation in answer\n",
      "accuracy compared to standard speculative decoding and over 2x with small loss\n",
      "in accuracy. When applied to the LiveCodeBench benchmark, our approach\n",
      "automatically detects other, programming-specific important tokens and shows\n",
      "similar speedups, demonstrating its ability to generalize across tasks.\n",
      "\n",
      "Link: http://arxiv.org/abs/2504.20039v1\n",
      "\n",
      "Title: Cam-2-Cam: Exploring the Design Space of Dual-Camera Interactions for\n",
      "  Smartphone-based Augmented Reality\n",
      "Authors: Brandon Woodard, Melvin He, Mose Sakashita, Jing Qian, Zainab Iftikhar, Joseph LaViola Jr\n",
      "Abstract:   Off-the-shelf smartphone-based AR systems typically use a single front-facing\n",
      "or rear-facing camera, which restricts user interactions to a narrow field of\n",
      "view and small screen size, thus reducing their practicality. We present\n",
      "\\textit{Cam-2-Cam}, an interaction concept implemented in three\n",
      "smartphone-based AR applications with interactions that span both cameras.\n",
      "Results from our qualitative analysis conducted on 30 participants presented\n",
      "two major design lessons that explore the interaction space of smartphone AR\n",
      "while maintaining critical AR interface attributes like embodiment and\n",
      "immersion: (1) \\textit{Balancing Contextual Relevance and Feedback Quality}\n",
      "serves to outline a delicate balance between implementing familiar interactions\n",
      "people do in the real world and the quality of multimodal AR responses and (2)\n",
      "\\textit{Preventing Disorientation using Simultaneous Capture and Alternating\n",
      "Cameras} which details how to prevent disorientation during AR interactions\n",
      "using the two distinct camera techniques we implemented in the paper.\n",
      "Additionally, we consider observed user assumptions or natural tendencies to\n",
      "inform future implementations of dual-camera setups for smartphone-based AR. We\n",
      "envision our design lessons as an initial pioneering step toward expanding the\n",
      "interaction space of smartphone-based AR, potentially driving broader adoption\n",
      "and overcoming limitations of single-camera AR.\n",
      "\n",
      "Link: http://arxiv.org/abs/2504.20035v1\n",
      "\n",
      "Title: More Clear, More Flexible, More Precise: A Comprehensive Oriented Object\n",
      "  Detection benchmark for UAV\n",
      "Authors: Kai Ye, Haidi Tang, Bowen Liu, Pingyang Dai, Liujuan Cao, Rongrong Ji\n",
      "Abstract:   Applications of unmanned aerial vehicle (UAV) in logistics, agricultural\n",
      "automation, urban management, and emergency response are highly dependent on\n",
      "oriented object detection (OOD) to enhance visual perception. Although existing\n",
      "datasets for OOD in UAV provide valuable resources, they are often designed for\n",
      "specific downstream tasks.Consequently, they exhibit limited generalization\n",
      "performance in real flight scenarios and fail to thoroughly demonstrate\n",
      "algorithm effectiveness in practical environments. To bridge this critical gap,\n",
      "we introduce CODrone, a comprehensive oriented object detection dataset for\n",
      "UAVs that accurately reflects real-world conditions. It also serves as a new\n",
      "benchmark designed to align with downstream task requirements, ensuring greater\n",
      "applicability and robustness in UAV-based OOD.Based on application\n",
      "requirements, we identify four key limitations in current UAV OOD datasets-low\n",
      "image resolution, limited object categories, single-view imaging, and\n",
      "restricted flight altitudes-and propose corresponding improvements to enhance\n",
      "their applicability and robustness.Furthermore, CODrone contains a broad\n",
      "spectrum of annotated images collected from multiple cities under various\n",
      "lighting conditions, enhancing the realism of the benchmark. To rigorously\n",
      "evaluate CODrone as a new benchmark and gain deeper insights into the novel\n",
      "challenges it presents, we conduct a series of experiments based on 22\n",
      "classical or SOTA methods.Our evaluation not only assesses the effectiveness of\n",
      "CODrone in real-world scenarios but also highlights key bottlenecks and\n",
      "opportunities to advance OOD in UAV applications.Overall, CODrone fills the\n",
      "data gap in OOD from UAV perspective and provides a benchmark with enhanced\n",
      "generalization capability, better aligning with practical applications and\n",
      "future algorithm development.\n",
      "\n",
      "Link: http://arxiv.org/abs/2504.20032v1\n",
      "\n",
      "Title: All-Subsets Important Separators with Applications to Sample Sets,\n",
      "  Balanced Separators and Vertex Sparsifiers in Directed Graphs\n",
      "Authors: Aditya Anand, Euiwoong Lee, Jason Li, Thatchaphol Saranurak\n",
      "Abstract:   Given a directed graph $G$ with $n$ vertices and $m$ edges, a parameter $k$\n",
      "and two disjoint subsets $S,T \\subseteq V(G)$, we show that the number of\n",
      "all-subsets important separators, which is the number of $A$-$B$ important\n",
      "vertex separators of size at most $k$ over all $A \\subseteq S$ and $B \\subseteq\n",
      "T$, is at most $\\beta(|S|, |T|, k) = 4^k {|S| \\choose \\leq k} {|T| \\choose \\leq\n",
      "2k}$, where ${x \\choose \\leq c} = \\sum_{i = 1}^c {x \\choose i}$, and that they\n",
      "can be enumerated in time $O(\\beta(|S|,|T|,k)k^2(m+n))$. This is a\n",
      "generalization of the folklore result stating that the number of $A$-$B$\n",
      "important separators for two fixed sets $A$ and $B$ is at most $4^k$ (first\n",
      "implicitly shown by Chen, Liu and Lu Algorithmica '09). From this result, we\n",
      "obtain the following applications: We give a construction for detection sets\n",
      "and sample sets in directed graphs, generalizing the results of Kleinberg\n",
      "(Internet Mathematics' 03) and Feige and Mahdian (STOC' 06) to directed graphs.\n",
      "Via our new sample sets, we give the first FPT algorithm for finding balanced\n",
      "separators in directed graphs parameterized by $k$, the size of the separator.\n",
      "Our algorithm runs in time $2^{O(k)} (m + n)$. We also give a $O({\\sqrt{\\log\n",
      "k}})$ approximation algorithm for the same problem. Finally, we present new\n",
      "results on vertex sparsifiers for preserving small cuts.\n",
      "\n",
      "Link: http://arxiv.org/abs/2504.20027v1\n",
      "\n",
      "Title: Better To Ask in English? Evaluating Factual Accuracy of Multilingual\n",
      "  LLMs in English and Low-Resource Languages\n",
      "Authors: Pritika Rohera, Chaitrali Ginimav, Gayatri Sawant, Raviraj Joshi\n",
      "Abstract:   Multilingual Large Language Models (LLMs) have demonstrated significant\n",
      "effectiveness across various languages, particularly in high-resource languages\n",
      "such as English. However, their performance in terms of factual accuracy across\n",
      "other low-resource languages, especially Indic languages, remains an area of\n",
      "investigation. In this study, we assess the factual accuracy of LLMs - GPT-4o,\n",
      "Gemma-2-9B, Gemma-2-2B, and Llama-3.1-8B - by comparing their performance in\n",
      "English and Indic languages using the IndicQuest dataset, which contains\n",
      "question-answer pairs in English and 19 Indic languages. By asking the same\n",
      "questions in English and their respective Indic translations, we analyze\n",
      "whether the models are more reliable for regional context questions in Indic\n",
      "languages or when operating in English. Our findings reveal that LLMs often\n",
      "perform better in English, even for questions rooted in Indic contexts.\n",
      "Notably, we observe a higher tendency for hallucination in responses generated\n",
      "in low-resource Indic languages, highlighting challenges in the multilingual\n",
      "understanding capabilities of current LLMs.\n",
      "\n",
      "Link: http://arxiv.org/abs/2504.20022v1\n",
      "\n",
      "Title: Modular Machine Learning: An Indispensable Path towards New-Generation\n",
      "  Large Language Models\n",
      "Authors: Xin Wang, Haoyang Li, Zeyang Zhang, Haibo Chen, Wenwu Zhu\n",
      "Abstract:   Large language models (LLMs) have dramatically advanced machine learning\n",
      "research including natural language processing, computer vision, data mining,\n",
      "etc., yet they still exhibit critical limitations in reasoning, factual\n",
      "consistency, and interpretability. In this paper, we introduce a novel learning\n",
      "paradigm -- Modular Machine Learning (MML) -- as an essential approach toward\n",
      "new-generation LLMs. MML decomposes the complex structure of LLMs into three\n",
      "interdependent components: modular representation, modular model, and modular\n",
      "reasoning, aiming to enhance LLMs' capability of counterfactual reasoning,\n",
      "mitigating hallucinations, as well as promoting fairness, safety, and\n",
      "transparency. Specifically, the proposed MML paradigm can: i) clarify the\n",
      "internal working mechanism of LLMs through the disentanglement of semantic\n",
      "components; ii) allow for flexible and task-adaptive model design; iii) enable\n",
      "interpretable and logic-driven decision-making process. We present a feasible\n",
      "implementation of MML-based LLMs via leveraging advanced techniques such as\n",
      "disentangled representation learning, neural architecture search and\n",
      "neuro-symbolic learning. We critically identify key challenges, such as the\n",
      "integration of continuous neural and discrete symbolic processes, joint\n",
      "optimization, and computational scalability, present promising future research\n",
      "directions that deserve further exploration. Ultimately, the integration of the\n",
      "MML paradigm with LLMs has the potential to bridge the gap between statistical\n",
      "(deep) learning and formal (logical) reasoning, thereby paving the way for\n",
      "robust, adaptable, and trustworthy AI systems across a wide range of real-world\n",
      "applications.\n",
      "\n",
      "Link: http://arxiv.org/abs/2504.20020v1\n",
      "\n",
      "Title: MINT: Multi-Vector Search Index Tuning\n",
      "Authors: Jiongli Zhu, Yue Wang, Bailu Ding, Philip A. Bernstein, Vivek Narasayya, Surajit Chaudhuri\n",
      "Abstract:   Vector search plays a crucial role in many real-world applications. In\n",
      "addition to single-vector search, multi-vector search becomes important for\n",
      "multi-modal and multi-feature scenarios today. In a multi-vector database, each\n",
      "row is an item, each column represents a feature of items, and each cell is a\n",
      "high-dimensional vector. In multi-vector databases, the choice of indexes can\n",
      "have a significant impact on performance. Although index tuning for relational\n",
      "databases has been extensively studied, index tuning for multi-vector search\n",
      "remains unclear and challenging. In this paper, we define multi-vector search\n",
      "index tuning and propose a framework to solve it. Specifically, given a\n",
      "multi-vector search workload, we develop algorithms to find indexes that\n",
      "minimize latency and meet storage and recall constraints. Compared to the\n",
      "baseline, our latency achieves 2.1X to 8.3X speedup.\n",
      "\n",
      "Link: http://arxiv.org/abs/2504.20018v1\n",
      "\n",
      "Title: Applying LLM-Powered Virtual Humans to Child Interviews in\n",
      "  Child-Centered Design\n",
      "Authors: Linshi Li, Hanlin Cai\n",
      "Abstract:   In child-centered design, directly engaging children is crucial for deeply\n",
      "understanding their experiences. However, current research often prioritizes\n",
      "adult perspectives, as interviewing children involves unique challenges such as\n",
      "environmental sensitivities and the need for trust-building. AI-powered virtual\n",
      "humans (VHs) offer a promising approach to facilitate engaging and multimodal\n",
      "interactions with children. This study establishes key design guidelines for\n",
      "LLM-powered virtual humans tailored to child interviews, standardizing\n",
      "multimodal elements including color schemes, voice characteristics, facial\n",
      "features, expressions, head movements, and gestures. Using ChatGPT-based prompt\n",
      "engineering, we developed three distinct Human-AI workflows (LLM-Auto,\n",
      "LLM-Interview, and LLM-Analyze) and conducted a user study involving 15\n",
      "children aged 6 to 12. The results indicated that the LLM-Analyze workflow\n",
      "outperformed the others by eliciting longer responses, achieving higher user\n",
      "experience ratings, and promoting more effective child engagement.\n",
      "\n",
      "Link: http://arxiv.org/abs/2504.20016v1\n",
      "\n",
      "Title: LLM-Generated Fake News Induces Truth Decay in News Ecosystem: A Case\n",
      "  Study on Neural News Recommendation\n",
      "Authors: Beizhe Hu, Qiang Sheng, Juan Cao, Yang Li, Danding Wang\n",
      "Abstract:   Online fake news moderation now faces a new challenge brought by the\n",
      "malicious use of large language models (LLMs) in fake news production. Though\n",
      "existing works have shown LLM-generated fake news is hard to detect from an\n",
      "individual aspect, it remains underexplored how its large-scale release will\n",
      "impact the news ecosystem. In this study, we develop a simulation pipeline and\n",
      "a dataset with ~56k generated news of diverse types to investigate the effects\n",
      "of LLM-generated fake news within neural news recommendation systems. Our\n",
      "findings expose a truth decay phenomenon, where real news is gradually losing\n",
      "its advantageous position in news ranking against fake news as LLM-generated\n",
      "news is involved in news recommendation. We further provide an explanation\n",
      "about why truth decay occurs from a familiarity perspective and show the\n",
      "positive correlation between perplexity and news ranking. Finally, we discuss\n",
      "the threats of LLM-generated fake news and provide possible countermeasures. We\n",
      "urge stakeholders to address this emerging challenge to preserve the integrity\n",
      "of news ecosystems.\n",
      "\n",
      "Link: http://arxiv.org/abs/2504.20013v1\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Research_Action_1\n",
      "\u001b[0m\n",
      "\u001b[33mResearch_Action_1\u001b[0m (to chat_manager):\n",
      "\n",
      "Here is a categorized markdown table of the LLM applications papers from last week:\n",
      "\n",
      "| Domain                          | Title                                                                 | Authors                                                                                                           | Summary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | Link                                      |\n",
      "|---------------------------------|-----------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------|\n",
      "| Computer Vision                 | Learning Streaming Video Representation via Multitask Training       | Yibin Yan, Jilan Xu, Shangzhe Di, Yikun Liu, Yudi Shi, Qirui Chen, Zeqian Li, Yifei Huang, Weidi Xie           | This paper presents StreamFormer, a novel streaming video backbone that incorporates causal temporal attention into a pre-trained vision transformer. It addresses challenges in real-time video processing and demonstrates competitive results in online action detection, video instance segmentation, and video question answering.                                                                                                                                                                                                                                           | [Link](http://arxiv.org/abs/2504.20041v1) |\n",
      "| Natural Language Processing      | AutoJudge: Judge Decoding Without Manual Annotation                   | Roman Garipov, Fedor Velikonivtsev, Ruslan Svirschevski, Vage Egiazarian, Max Ryabinin                          | AutoJudge is a framework that accelerates LLM inference using task-specific lossy speculative decoding. It identifies unimportant tokens to generate faster while maintaining quality, achieving significant speedups in token acceptance with minimal degradation in accuracy.                                                                                                                                                                                                                                                                                     | [Link](http://arxiv.org/abs/2504.20039v1) |\n",
      "| Augmented Reality               | Cam-2-Cam: Exploring the Design Space of Dual-Camera Interactions   | Brandon Woodard, Melvin He, Mose Sakashita, Jing Qian, Zainab Iftikhar, Joseph LaViola Jr                      | This paper introduces Cam-2-Cam, an interaction concept for smartphone-based AR that utilizes both front and rear cameras. It discusses design lessons to enhance user interactions and prevent disorientation, aiming to expand the interaction space of smartphone AR.                                                                                                                                                                                                                                                                                     | [Link](http://arxiv.org/abs/2504.20035v1) |\n",
      "| UAV Applications                | More Clear, More Flexible, More Precise: A Comprehensive Oriented Object Detection benchmark for UAV | Kai Ye, Haidi Tang, Bowen Liu, Pingyang Dai, Liujuan Cao, Rongrong Ji                                          | CODrone is introduced as a comprehensive dataset for oriented object detection in UAVs, addressing limitations of existing datasets. It aims to enhance generalization performance in real-world scenarios and provides a new benchmark for evaluating UAV-based OOD algorithms.                                                                                                                                                                                                                                                                               | [Link](http://arxiv.org/abs/2504.20032v1) |\n",
      "| Graph Theory                   | All-Subsets Important Separators with Applications to Sample Sets    | Aditya Anand, Euiwoong Lee, Jason Li, Thatchaphol Saranurak                                                   | This paper presents a generalization of important vertex separators in directed graphs and provides applications for detection sets and balanced separators. It introduces a new FPT algorithm for finding balanced separators and discusses vertex sparsifiers for preserving small cuts.                                                                                                                                                                                                                                                                       | [Link](http://arxiv.org/abs/2504.20027v1) |\n",
      "| Multilingual LLMs              | Better To Ask in English? Evaluating Factual Accuracy of Multilingual LLMs in English and Low-Resource Languages | Pritika Rohera, Chaitrali Ginimav, Gayatri Sawant, Raviraj Joshi                                              | This study evaluates the factual accuracy of multilingual LLMs by comparing their performance in English and low-resource Indic languages. It finds that LLMs generally perform better in English, highlighting challenges in multilingual understanding and a tendency for hallucination in low-resource languages.                                                                                                                                                                                                                                           | [Link](http://arxiv.org/abs/2504.20022v1) |\n",
      "| Modular Machine Learning        | Modular Machine Learning: An Indispensable Path towards New-Generation Large Language Models | Xin Wang, Haoyang Li, Zeyang Zhang, Haibo Chen, Wenwu Zhu                                                    | The paper introduces Modular Machine Learning (MML) as a paradigm to enhance LLMs' capabilities in reasoning and interpretability. It discusses the decomposition of LLMs into modular components and presents challenges and future research directions for integrating MML with LLMs to improve AI systems.                                                                                                                                                                                                                                           | [Link](http://arxiv.org/abs/2504.20020v1) |\n",
      "| Database Systems                | MINT: Multi-Vector Search Index Tuning                               | Jiongli Zhu, Yue Wang, Bailu Ding, Philip A. Bernstein, Vivek Narasayya, Surajit Chaudhuri                     | This paper defines multi-vector search index tuning and proposes a framework to optimize indexes for multi-vector databases. It presents algorithms that significantly improve latency while meeting storage and recall constraints, achieving notable speedups compared to baseline methods.                                                                                                                                                                                                                                                                 | [Link](http://arxiv.org/abs/2504.20018v1) |\n",
      "| Child-Centered Design          | Applying LLM-Powered Virtual Humans to Child Interviews in Child-Centered Design | Linshi Li, Hanlin Cai                                                                                          | The study explores the use of LLM-powered virtual humans for child interviews, establishing design guidelines for effective engagement. It presents three workflows and finds that the LLM-Analyze workflow significantly enhances user experience and child engagement during interviews.                                                                                                                                                                                                                                                                         | [Link](http://arxiv.org/abs/2504.20016v1) |\n",
      "| Fake News Detection            | LLM-Generated Fake News Induces Truth Decay in News Ecosystem: A Case Study on Neural News Recommendation | Beizhe Hu, Qiang Sheng, Juan Cao, Yang Li, Danding Wang                                                      | This study investigates the impact of LLM-generated fake news on news ecosystems, revealing a truth decay phenomenon where real news loses ranking against fake news. It discusses the implications of this trend and suggests countermeasures to preserve news integrity.                                                                                                                                                                                                                                                                                     | [Link](http://arxiv.org/abs/2504.20013v1) |\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> TERMINATING RUN (79cf8da2-fd64-4afb-a00d-5cf7599e4e84): No next speaker selected\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "chat_result = initializer.initiate_chat(\n",
    "    manager, message=\"Topic: LLM applications papers from last week. Requirement: 5 -10 papers from different domains.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "user-prediction-jY4gRI11-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
